name: AI Chatbot Testing Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      endpoint:
        description: 'API Endpoint URL'
        required: false
        default: 'https://api.example.com'
      paper_file:
        description: 'Research paper file (in data/ directory)'
        required: false
        default: 'sample_paper.txt'
      concurrent_requests:
        description: 'Number of concurrent requests'
        required: false
        default: '3'

env:
  PYTHON_VERSION: '3.9'

jobs:
  setup:
    name: Setup and Validation
    runs-on: ubuntu-latest
    outputs:
      endpoint: ${{ steps.config.outputs.endpoint }}
      paper_file: ${{ steps.config.outputs.paper_file }}
      concurrent_requests: ${{ steps.config.outputs.concurrent_requests }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Configure parameters
      id: config
      run: |
        # Set endpoint URL from input or secret
        if [ "${{ github.event.inputs.endpoint }}" != "" ]; then
          echo "endpoint=${{ github.event.inputs.endpoint }}" >> $GITHUB_OUTPUT
        elif [ "${{ secrets.CHATBOT_ENDPOINT }}" != "" ]; then
          echo "endpoint=${{ secrets.CHATBOT_ENDPOINT }}" >> $GITHUB_OUTPUT
        else
          echo "endpoint=https://api.example.com" >> $GITHUB_OUTPUT
        fi
        
        # Set paper file
        echo "paper_file=${{ github.event.inputs.paper_file || 'sample_paper.txt' }}" >> $GITHUB_OUTPUT
        
        # Set concurrent requests
        echo "concurrent_requests=${{ github.event.inputs.concurrent_requests || '3' }}" >> $GITHUB_OUTPUT
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
    
    - name: Validate scripts syntax
      run: |
        python -m py_compile scripts/generate_questions.py
        python -m py_compile scripts/run_tests.py
        python -m py_compile scripts/judge.py
        python -m py_compile scripts/report.py
        echo "‚úÖ All scripts have valid syntax"
    
    - name: Validate data files
      run: |
        if [ ! -f "data/${{ steps.config.outputs.paper_file }}" ]; then
          echo "‚ùå Paper file 'data/${{ steps.config.outputs.paper_file }}' not found"
          exit 1
        fi
        echo "‚úÖ Data files validated"

  generate-questions:
    name: Generate Test Questions
    runs-on: ubuntu-latest
    needs: setup
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
    
    - name: Generate test questions
      run: |
        cd scripts
        python generate_questions.py \
          --input "../data/${{ needs.setup.outputs.paper_file }}" \
          --output "../data/test_data.jsonl" \
          --title "Research Paper for CI/CD Testing" \
          --verbose
    
    - name: Validate generated questions
      run: |
        # Check if file was created and contains data
        if [ ! -f "data/test_data.jsonl" ]; then
          echo "‚ùå Test data file was not generated"
          exit 1
        fi
        
        # Count number of questions
        question_count=$(wc -l < data/test_data.jsonl)
        echo "Generated $question_count test questions"
        
        if [ "$question_count" -lt 10 ]; then
          echo "‚ùå Too few questions generated (minimum 10 expected)"
          exit 1
        fi
        
        # Validate JSON format
        python -c "
        import json
        line_count = 0
        with open('data/test_data.jsonl', 'r') as f:
            for line in f:
                if line.strip():
                    json.loads(line)
                    line_count += 1
        print(f'‚úÖ All {line_count} questions have valid JSON format')
        "
    
    - name: Upload test questions
      uses: actions/upload-artifact@v4
      with:
        name: test-questions
        path: data/test_data.jsonl
        retention-days: 30

  run-tests:
    name: Execute API Tests
    runs-on: ubuntu-latest
    needs: [setup, generate-questions]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
    
    - name: Download test questions
      uses: actions/download-artifact@v4
      with:
        name: test-questions
        path: data/
    
    - name: Run API tests
      env:
        CHATBOT_API_KEY: ${{ secrets.CHATBOT_API_KEY }}
        CHATBOT_ENDPOINT: ${{ needs.setup.outputs.endpoint }}
      run: |
        cd scripts
        python run_tests.py \
          --tests "../data/test_data.jsonl" \
          --paper "../data/${{ needs.setup.outputs.paper_file }}" \
          --endpoint "${{ needs.setup.outputs.endpoint }}" \
          --output "../data/raw_results.jsonl" \
          --concurrent "${{ needs.setup.outputs.concurrent_requests }}" \
          --timeout 45 \
          --retries 3 \
          --verbose
    
    - name: Validate test results
      run: |
        # Check if results file was created
        if [ ! -f "data/raw_results.jsonl" ]; then
          echo "‚ùå Test results file was not generated"
          exit 1
        fi
        
        # Count results and analyze success rate
        python -c "
        import json
        results = []
        with open('data/raw_results.jsonl', 'r') as f:
            for line in f:
                if line.strip():
                    results.append(json.loads(line))
        
        total = len(results)
        successful = len([r for r in results if r.get('status') == 'success'])
        failed = total - successful
        success_rate = (successful / total) * 100 if total > 0 else 0
        
        print(f'Test Results: {successful}/{total} successful ({success_rate:.1f}%)')
        
        if success_rate < 70:
            print(f'‚ùå Success rate too low: {success_rate:.1f}% (minimum 70% expected)')
            exit(1)
        else:
            print(f'‚úÖ Success rate acceptable: {success_rate:.1f}%')
        "
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      with:
        name: test-results
        path: data/raw_results.jsonl
        retention-days: 30

  evaluate-responses:
    name: Evaluate Responses
    runs-on: ubuntu-latest
    needs: [setup, run-tests]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
    
    - name: Download test results
      uses: actions/download-artifact@v4
      with:
        name: test-results
        path: data/
    
    - name: Evaluate responses
      run: |
        cd scripts
        python judge.py \
          --results "../data/raw_results.jsonl" \
          --paper "../data/${{ needs.setup.outputs.paper_file }}" \
          --output "../data/evaluated.jsonl" \
          --verbose
    
    - name: Analyze evaluation results
      id: analysis
      run: |
        python -c "
        import json
        evaluations = []
        with open('data/evaluated.jsonl', 'r') as f:
            for line in f:
                if line.strip():
                    evaluations.append(json.loads(line))
        
        if not evaluations:
            print('‚ùå No evaluations found')
            exit(1)
        
        # Calculate averages
        avg_relevancy = sum(e.get('relevancy', 0) for e in evaluations) / len(evaluations)
        avg_adequacy = sum(e.get('adequacy', 0) for e in evaluations) / len(evaluations)
        avg_clarity = sum(e.get('clarity', 0) for e in evaluations) / len(evaluations)
        avg_confusion = sum(e.get('confusion', 0) for e in evaluations) / len(evaluations)
        avg_overall = (avg_relevancy + avg_adequacy + avg_clarity + avg_confusion) / 4
        
        print(f'üìä Evaluation Results:')
        print(f'   Relevancy: {avg_relevancy:.2f}/5')
        print(f'   Adequacy: {avg_adequacy:.2f}/5')
        print(f'   Clarity: {avg_clarity:.2f}/5')
        print(f'   Consistency: {avg_confusion:.2f}/5')
        print(f'   Overall: {avg_overall:.2f}/5')
        
        # Set outputs for summary
        with open('${{ github.env }}', 'a') as env_file:
            env_file.write(f'AVG_OVERALL={avg_overall:.2f}\n')
            env_file.write(f'AVG_RELEVANCY={avg_relevancy:.2f}\n')
            env_file.write(f'AVG_ADEQUACY={avg_adequacy:.2f}\n')
            env_file.write(f'AVG_CLARITY={avg_clarity:.2f}\n')
            env_file.write(f'AVG_CONSISTENCY={avg_confusion:.2f}\n')
        
        # Check if scores meet minimum thresholds
        if avg_overall < 3.0:
            print(f'‚ùå Overall score too low: {avg_overall:.2f} (minimum 3.0 expected)')
            exit(1)
        else:
            print(f'‚úÖ Overall score acceptable: {avg_overall:.2f}')
        "
    
    - name: Upload evaluations
      uses: actions/upload-artifact@v4
      with:
        name: evaluations
        path: data/evaluated.jsonl
        retention-days: 30

  generate-reports:
    name: Generate Reports
    runs-on: ubuntu-latest
    needs: [setup, run-tests, evaluate-responses]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
    
    - name: Download artifacts
      uses: actions/download-artifact@v4
      with:
        name: test-results
        path: data/
    
    - name: Download evaluations
      uses: actions/download-artifact@v4
      with:
        name: evaluations
        path: data/
    
    - name: Generate reports
      run: |
        cd scripts
        python report.py \
          --evaluations "../data/evaluated.jsonl" \
          --results "../data/raw_results.jsonl" \
          --output-dir "../reports" \
          --format all \
          --verbose
    
    - name: List generated reports
      run: |
        echo "üìã Generated Reports:"
        ls -la reports/
    
    - name: Upload reports
      uses: actions/upload-artifact@v4
      with:
        name: test-reports
        path: reports/
        retention-days: 90

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [setup, run-tests, evaluate-responses, generate-reports]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
    
    - name: Create summary
      run: |
        echo "# ü§ñ AI Chatbot Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "test-results/raw_results.jsonl" ]; then
          # Analyze test execution
          python3 -c "
          import json
          results = []
          with open('test-results/raw_results.jsonl', 'r') as f:
              for line in f:
                  if line.strip():
                      results.append(json.loads(line))
          
          total = len(results)
          successful = len([r for r in results if r.get('status') == 'success'])
          success_rate = (successful / total) * 100 if total > 0 else 0
          
          print(f'## üöÄ Test Execution')
          print(f'- **Total Tests**: {total}')
          print(f'- **Successful**: {successful} ({success_rate:.1f}%)')
          print(f'- **Failed**: {total - successful}')
          print()
          " >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "evaluations/evaluated.jsonl" ]; then
          # Analyze evaluations
          python3 -c "
          import json
          evaluations = []
          with open('evaluations/evaluated.jsonl', 'r') as f:
              for line in f:
                  if line.strip():
                      evaluations.append(json.loads(line))
          
          if evaluations:
              avg_relevancy = sum(e.get('relevancy', 0) for e in evaluations) / len(evaluations)
              avg_adequacy = sum(e.get('adequacy', 0) for e in evaluations) / len(evaluations)
              avg_clarity = sum(e.get('clarity', 0) for e in evaluations) / len(evaluations)
              avg_confusion = sum(e.get('confusion', 0) for e in evaluations) / len(evaluations)
              avg_overall = (avg_relevancy + avg_adequacy + avg_clarity + avg_confusion) / 4
              
              print(f'## üìä Quality Metrics (0-5 scale)')
              print(f'- **Relevancy**: {avg_relevancy:.2f}')
              print(f'- **Adequacy**: {avg_adequacy:.2f}')
              print(f'- **Clarity**: {avg_clarity:.2f}')
              print(f'- **Consistency**: {avg_confusion:.2f}')
              print(f'- **Overall Score**: {avg_overall:.2f}')
              print()
          " >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "## üìÅ Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Test Questions, Results, Evaluations, and Reports are available as workflow artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Reports include CSV data, HTML dashboard, and JSON summaries" >> $GITHUB_STEP_SUMMARY
        
        # Add configuration info
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ‚öôÔ∏è Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **API Endpoint**: ${{ needs.setup.outputs.endpoint }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Paper File**: ${{ needs.setup.outputs.paper_file }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Concurrent Requests**: ${{ needs.setup.outputs.concurrent_requests }}" >> $GITHUB_STEP_SUMMARY

  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [setup, run-tests, evaluate-responses, summary]
    if: always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')
    
    steps:
    - name: Notify on failure
      if: failure()
      run: |
        echo "üö® AI Chatbot tests failed!"
        echo "Check the workflow summary and artifacts for details."
        # Add Slack/email notification here if desired
    
    - name: Notify on success  
      if: success()
      run: |
        echo "‚úÖ AI Chatbot tests passed successfully!"
        echo "All quality metrics are within acceptable ranges."
